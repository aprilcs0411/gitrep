---
title: "text mining and world maps¡°
author: "Your name here"
date: "Date here"
---

# Load, clean, and wrangle data

```{r load-packages-data, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
library(gutenbergr)
library(sf)

# Load and clean RIAA data
# https://www.riaa.com/u-s-sales-database/
riaa <- read_csv("riaa.csv") %>% 
  # Only look at these kinds of sales
  filter(Format %in% c("LP/EP", "Cassette", "CD", "Download Album", 
                       "Download Single", "Ringtones & Ringbacks", 
                       "On-Demand Streaming (Ad-Supported)", "Paid Subscription")) %>% 
  # Look at inflation-adjusted sales (the other metrics are for
  # non-inflation-adjusted sales and actual units sold)
  filter(Metric == "Value (Adjusted)") %>% 
  mutate(Value = ifelse(is.na(Value), 0, Value))


```

```{r get-gutenberg-data}
# TODO: Use the gutenberg_download() function to download a bunch of books

# BONUS: If you don't want to redownload these books every time you knit this
# document, use write_csv() to save a CSV version of the book data frame to your
# data folder. Then use read_csv() to load that data instead of gutenberg_download()
```

# Task 1: RIAA music revenues

Do stuff here. 

Note that these values are adjusted for inflation and represent 2017 dollars. Also, try moving beyond the default colors and consider adding labels directly to the plot rather than using a legend.

Tell a story about what's happening in this chart. Interpret it.

```{r warning=FALSE, message=FALSE}
library(ggplot2)
library(patchwork)
```
Generally, almost every format of music is losing.
```{r}
ggplot(riaa,aes(x=Year,y=Value))+geom_point()+facet_wrap(~Format)
```

Someone had its times. However, that time has gone.

```{r}
ggplot(riaa,aes(x=Year,y=Value))+geom_point()+facet_wrap(~Format)
p1<-ggplot(riaa,aes(x=Year,y=Value))+geom_point(aes(color=factor(Format),alpha=0.5))+guides(color=guide_legend(title=""),alpha=FALSE)
p1
allmusic<-riaa%>%group_by(Year)%>%summarize(total=sum(Value))
p2<-ggplot(allmusic,aes(x=Year,y=total))+geom_point(aes(color="red",alpha=0.9))+geom_smooth()+guides(colour=FALSE,alpha=FALSE)#+geom_text(aes(label=paste(total,"$",sep="")),vjust=-0.5)
p2
p1+p2+plot_layout(ncol = 1)

```
The only booming one is this.
```{r}
b<-riaa%>%filter(Format=="Paid Subscription",Year>=2005)
b%>%ggplot(aes(x=Year,y=Value))+geom_point()+geom_smooth()
```

Is it possible that, one day, paid subscription will trigger a potential boom in music industry?
```{r}
library(MASS)
da<-b[,c("Value","Year")]
trans<-boxcox(da$Value~da$Year)
lam<-trans$x[which.max(trans$y)]
lam
lambda=-0.75
Y=da$Value
Ylam<- (Y^lambda-1)/lambda
da$Value<-Ylam
fit<-lm(da$Value~da$Year)
re<-data.frame(fit$fitted.values,da$Value)
re

beta0<-fit$coefficients[1]
beta1<-fit$coefficients[2]
x<-seq(2005,2018,by=1)
lamy=beta0+beta1*x
y=(1+lambda*lamy)^(1/lambda)
plot(y~x)

```
i try to do some math. When time pass 2018, the prediction becoms odd. Since i have so little points, less than 30, the result proves nothing. More data, please.

# Task 2: World map

Do stuff here.

Tell a story about what's happening in this map. Interpret it.

```{r plot-2015-internet-users}
library(scico)
internet_user<-read.csv("share-of-individuals-using-the-internet-1990-2015.csv")%>%rename(users = Individuals.using.the.Internet....of.population.....of.population.,
         ISO_A3 = Code)
world_shapes <- st_read("ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp",
                        stringsAsFactors = FALSE)


# Only look at 2015
users_2015 <- internet_user %>%
  filter(Year == 2015)

# left_join takes two data frames and combines them, based on a shared column
# (in this case ISO_A3)
users_map <- world_shapes %>%
  left_join(users_2015, by = "ISO_A3") %>%
  filter(ISO_A3 != "ATA")  # No internet in Antarctica. 

# TODO: Make a map of internet users with ggplot() + geom_sf()
ggplot() +
  geom_sf(data = users_map, aes(fill = users),  size = 0.1) +
  scale_fill_scico(palette = "bilbao", end = 0.9, begin = 0.05,
                   labels = scales::comma, na.value = "white",
                   name = "Proportion of internet users") +
  coord_sf(crs = 3785, datum = NA) +  # Mercator
  # coord_sf(crs = "+proj=merc", datum = NA) +  # You can also do this
 guides(fill = guide_colorbar(barwidth = 4, barheight = 10)) +  # 
  labs(title = "internet user in 2015") +
  theme_minimal() +
  theme(axis.text.x = element_blank())

```
#
internet prosperity in each country. (just an example, due to data deficiency )
```{r}
usersin2010s <- internet_user %>%filter(Year>=2010)%>%group_by(ISO_A3)%>%summarize(k=lm(users~Year)$coefficients[2])

users_map_growing <- world_shapes %>%
  left_join(usersin2010s, by = "ISO_A3") %>%
  filter(ISO_A3 != "ATA",ISO_A3!="SMR")

ggplot() +
  geom_sf(data = users_map_growing , aes(fill = k),  size = 0.1) +
  scale_fill_gradient(low = "#FFF3EE", high = "#FF8040",labels = scales::comma, na.value = "white",
                   name = "growing speed of internet users")  +
  coord_sf(crs = 3785, datum = NA) +  # Mercator
  # coord_sf(crs = "+proj=merc", datum = NA) +  # You can also do this
 guides(fill = guide_colorbar(barwidth = 4, barheight = 10)) +  # 
  labs(title = "internet users booming") +
  theme_minimal() +
  theme(axis.text.x = element_blank())
```


# Task 3: Personal map

Do stuff here. Tell a story about what's happening in this map.


# Task 4: Word frequencies
different volume of 3 books.


```{r}
library(tidyverse)
library(tidytext)
library(gutenbergr)
library(topicmodels)
library(cleanNLP)
library(wordcloud)
leviathan<-read_lines("Leviathan.txt",n_max=-1L)
text<-paste(leviathan,collapse =" ")
txt<-unlist(strsplit(text, split=" "))
leviathan<-data.frame(txt=txt,stringsAsFactors = FALSE)


le_words<-leviathan%>%unnest_tokens(output=word,input=txt,token="words",to_lower = TRUE)%>%anti_join(stop_words)%>%count(word,sort = TRUE)%>%top_n(100)%>%mutate(word=fct_inorder(word))

wordcloud(le_words$word,le_words$n,min.freq = 10,random.order = FALSE)


```



## Top 15 most frequent words in each book

```{r}
leviathan<-read_lines("Leviathan.txt",n_max=-1L)
text<-paste(leviathan,collapse =" ")
le_txt<-unlist(strsplit(text, split=" "))
leviathan<-data.frame(txt=le_txt,stringsAsFactors = FALSE)


le_words<-leviathan%>%unnest_tokens(output=word,input=txt,token="words",to_lower = TRUE)%>%anti_join(stop_words)%>%count(word,sort = TRUE)%>%top_n(15)%>%mutate(word=fct_inorder(word))


ggplot(le_words, aes(x = fct_rev(word), y = n)) + 
  geom_col() + 
  coord_flip() +
  scale_y_continuous(labels = scales::comma) +
  labs(y = "Count", x = NULL, title = "15 most frequent words in the Book of Leviathan") +
  theme_minimal()

le_bigrams <- leviathan %>% 
  unnest_tokens(bigram, txt, token = "ngrams", n = 2) %>% 
  # Split the bigram column into two columns
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>% 
  # Put the two word columns back together
  unite(bigram, word1, word2, sep = " ") %>% 
  count(bigram, sort = TRUE) %>% 
  top_n(10)
```

## Top 10 most unique words in each book
```{r}
leviathan<-read_lines("Leviathan.txt",n_max=-1L)
le_text<-paste(leviathan,collapse =" ")

pride<-read_lines("PRIDE AND PREJUDICE.txt",n_max=-1L)
pride_text<-paste(pride,collapse =" ")


CC<-read_lines("A Christmas Carol.txt",n_max=-1L)
CC_text<-paste(CC,collapse =" ")


three_books=data.frame(book_title=c("pride","CC","leviathan"),text=c(pride_text,CC_text,le_text),stringsAsFactors = FALSE)
three_words<-three_books%>%unnest_tokens(word,text)%>%anti_join(stop_words)%>%count(book_title,word,sort = TRUE)%>%ungroup()

words_tf_idf <- three_words %>% 
  bind_tf_idf(word, book_title, n) %>% 
  arrange(desc(tf_idf))


words_tf_idf_plot <- words_tf_idf %>% 
  group_by(book_title) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = fct_inorder(word))

p1<-ggplot(words_tf_idf_plot, aes(x = fct_rev(word), y = tf_idf, fill = book_title)) +
  geom_col() +
  guides(fill = FALSE) +
  labs(y = "tf-idf", x = NULL) +
  facet_wrap(~ book_title, scales = "free") +
  theme_minimal() +
  coord_flip()

p2<-ggplot(words_tf_idf_plot, aes(x = fct_rev(word), y = n, fill = book_title)) +
  geom_col() +
  guides(fill = FALSE) +
  labs(y = "tf", x = NULL) +
  facet_wrap(~ book_title, scales = "free") +
  theme_minimal() +
  coord_flip()
library(patchwork)
p1+p2+plot_layout(ncol = 1)
```


